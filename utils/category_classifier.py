# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
"""
import re
import asyncio
from typing import List, Dict, Tuple, Optional
from collections import defaultdict, Counter
import math
from datetime import datetime
import pickle
import os

try:
    from pymorphy3 import MorphAnalyzer
except ImportError:
    MorphAnalyzer = None

from models.transaction import TransactionData
from models.keyword_dictionary import KeywordDictionary
from config import logger, KEYWORDS_SPREADSHEET_ID, KEYWORDS_SHEET_NAME

MODEL_FILE_PATH = "category_classifier_model.pkl"


class TransactionCategoryClassifier:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    """
    def __init__(self, keyword_dict: Optional[KeywordDictionary] = None):
        self.category_keywords = defaultdict(list)  # –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.category_features = defaultdict(lambda: defaultdict(int))  # —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        self.global_features = defaultdict(int)  # —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≥–ª–æ–±–∞–ª—å–Ω–æ
        self.category_transactions_count = defaultdict(int)  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.total_transactions = 0
        self.categories = set()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MorphAnalyzer –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        if MorphAnalyzer:
            try:
                self.morph_analyzer = MorphAnalyzer()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å pymorphy3 MorphAnalyzer: {e}")
                self.morph_analyzer = None
        else:
            logger.warning("‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            self.morph_analyzer = None
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π KeywordDictionary
        if keyword_dict is None:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä KeywordDictionary
            self.keyword_dict = KeywordDictionary(
                spreadsheet_id=KEYWORDS_SPREADSHEET_ID,
                sheet_name=KEYWORDS_SHEET_NAME
            )
        else:
            self.keyword_dict = keyword_dict
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –µ—Å—Ç—å
        self.load_model()
            
    async def load(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è KeywordDictionary"""
        try:
            # –ï—Å–ª–∏ keyword_dict –µ—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç, –¥–µ–ª–∞–µ–º —ç—Ç–æ
            if not hasattr(self.keyword_dict, 'async_load_from_sheets'):
                self.keyword_dict = KeywordDictionary(
                    spreadsheet_id=KEYWORDS_SPREADSHEET_ID,
                    sheet_name=KEYWORDS_SHEET_NAME
                )
            
            # –í—ã–∑—ã–≤–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
            await self.keyword_dict.load()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ KeywordDictionary: {e}")

    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª"""
        try:
            model_state = {
                'category_features': dict(self.category_features),  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º defaultdict –≤ dict –¥–ª—è pickle
                'global_features': dict(self.global_features),
                'category_transactions_count': dict(self.category_transactions_count),
                'categories': self.categories,
                'total_transactions': self.total_transactions
            }
            with open(MODEL_FILE_PATH, 'wb') as f:
                pickle.dump(model_state, f)
            logger.info(f"üíæ ML-–º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_FILE_PATH}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(MODEL_FILE_PATH):
            return

        try:
            with open(MODEL_FILE_PATH, 'rb') as f:
                model_state = pickle.load(f)
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º defaultdict'—ã
            self.category_features = defaultdict(lambda: defaultdict(int))
            for cat, features in model_state.get('category_features', {}).items():
                self.category_features[cat] = defaultdict(int, features)
                
            self.global_features = defaultdict(int, model_state.get('global_features', {}))
            self.category_transactions_count = defaultdict(int, model_state.get('category_transactions_count', {}))
            self.categories = model_state.get('categories', set())
            self.total_transactions = model_state.get('total_transactions', 0)
            
            logger.info(f"üìÇ ML-–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {MODEL_FILE_PATH} ({self.total_transactions} trx)")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
         
    def extract_features(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        text = text.lower()
        # –£–±–∏—Ä–∞–µ–º —Ü–∏—Ñ—Ä—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text)
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω morph_analyzer
        if self.morph_analyzer:
            lemmatized_words = []
            for word in words:
                if len(word) >= 2:  # –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π 2 —Å–∏–º–≤–æ–ª–∞ –∏ –±–æ–ª–µ–µ
                    try:
                        parsed_word = self.morph_analyzer.parse(word)[0]
                        lemma = parsed_word.normal_form
                        lemmatized_words.append(lemma)
                    except Exception:
                        # –ï—Å–ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ
                        lemmatized_words.append(word)
            words = lemmatized_words
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º n-–≥—Ä–∞–º–º—ã
        features = []
        for word in words:
            if len(word) >= 3:  # –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π 3 —Å–∏–º–≤–æ–ª–∞ –∏ –±–æ–ª–µ–µ
                features.append(word)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∏–≥—Ä–∞–º–º—ã (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 2 —Å–ª–æ–≤)
        for i in range(len(words) - 1):
            if len(words[i]) >= 2 and len(words[i+1]) >= 2:
                bigram = f"{words[i]}_{words[i+1]}"
                features.append(bigram)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        return list(set(features))
    
    def lemmatize_word(self, word: str) -> str:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        """
        if self.morph_analyzer and len(word) >= 2:
            try:
                parsed_word = self.morph_analyzer.parse(word)[0]
                return parsed_word.normal_form
            except Exception:
                # –ï—Å–ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ
                return word
        return word
    
    def lemmatize_text(self, text: str) -> str:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if not self.morph_analyzer:
            return text.lower()
            
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())
        lemmatized_words = [self.lemmatize_word(word) for word in words]
        return ' '.join(lemmatized_words)
    
    def train(self, transactions: List[TransactionData]):
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ {len(transactions)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö")
        
        for transaction in transactions:
            category = transaction.category
            self.categories.add(category)
            self.category_transactions_count[category] += 1
            self.total_transactions += 1
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è, –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–≤—Ü–∞ –∏ —Å–ø–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤
            text = f"{transaction.comment} {transaction.retailer_name} {transaction.items_list}"
            features = self.extract_features(text)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            for feature in features:
                self.category_features[category][feature] += 1
                self.global_features[feature] += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for category in self.categories:
            category_features = self.category_features[category]
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            sorted_features = sorted(
                category_features.items(), 
                key=lambda x: self._calculate_tfidf(x[0], category), 
                reverse=True
            )
            self.category_keywords[category] = [feature for feature, _ in sorted_features[:10]]
        
        
        logger.info(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(self.categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        self.save_model()
    
    def _calculate_tfidf(self, feature: str, category: str) -> float:
        """
        –†–∞—Å—á–µ—Ç TF-IDF –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        """
        # Term Frequency –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_sum = sum(self.category_features[category].values())
        if category_sum == 0:
            tf = 0  # –ï—Å–ª–∏ —Å—É–º–º–∞ —Ä–∞–≤–Ω–∞ –Ω—É–ª—é, —Ç–æ –∏ —á–∞—Å—Ç–æ—Ç–∞ —Ä–∞–≤–Ω–∞ –Ω—É–ª—é
        else:
            tf = self.category_features[category][feature] / category_sum
        
        # Inverse Document Frequency
        category_containing_feature = sum(
            1 for cat in self.categories
            if self.category_features[cat][feature] > 0
        )
        idf = math.log(self.total_transactions / category_containing_feature) if category_containing_feature > 0 else 0
        
        return tf * idf
    
    def predict_category(self, transaction: TransactionData) -> Tuple[str, float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –Ω–æ–≤–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        """
        text = f"{transaction.comment} {transaction.retailer_name} {transaction.items_list}"
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if hasattr(self, 'keyword_dict'):
            keyword_result = self.keyword_dict.get_category_by_keyword(text)
            if keyword_result:
                return keyword_result

        # 2. –ï—Å–ª–∏ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º ML
        features = self.extract_features(text)
        
        scores = {}
        has_matching_features = False
        
        for category in self.categories:
            # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_score = 0
            category_total_features = sum(self.category_features[category].values())
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫
            for feature in features:
                if category_total_features > 0:
                    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –¥–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    if self.category_features[category][feature] > 0:
                        has_matching_features = True
                        feature_prob = self.category_features[category][feature] / category_total_features
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ—Ü–µ–Ω–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF
                        tfidf = self._calculate_tfidf(feature, category)
                        category_score += feature_prob * (1 + tfidf)
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º –∞–ø—Ä–∏–æ—Ä–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            prior_prob = self.category_transactions_count[category] / self.total_transactions if self.total_transactions > 0 else 0
            scores[category] = category_score + prior_prob
        
        if not scores:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é
            # –ù–æ —Å –Ω—É–ª–µ–≤–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é, —á—Ç–æ–±—ã –Ω–µ –ø–æ–¥—Å—Ç–∞–≤–ª—è—Ç—å –µ—ë –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–æ
            if self.category_transactions_count:
                most_common_category = max(self.category_transactions_count, key=self.category_transactions_count.get)
                return most_common_category, 0.0 # –ë—ã–ª–æ 0.5, —Ç–µ–ø–µ—Ä—å 0.0
            else:
                return "–ü—Ä–æ—á–µ–µ –†–∞—Å—Ö–æ–¥", 0.0
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π
        best_category = max(scores, key=scores.get)
        max_score = scores[best_category]
        
        # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º, –∑–Ω–∞—á–∏—Ç —Å—Ä–∞–±–æ—Ç–∞–ª–∞ —Ç–æ–ª—å–∫–æ –∞–ø—Ä–∏–æ—Ä–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (prior_prob)
        # –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –º—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–≤–µ—Ä–µ–Ω—ã –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ
        if not has_matching_features:
            return best_category, 0.0

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫—É –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        if max_score > 0:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º softmax-–ø–æ–¥–æ–±–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            total_score = sum(scores.values())
            confidence = max_score / total_score if total_score > 0 else 0.0
        else:
            confidence = 0.0
            
        return best_category, confidence
    
    def suggest_category_with_improvement(self, transaction: TransactionData) -> Tuple[str, float]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å —É—Ä–æ–≤–Ω–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.
        """
        return self.predict_category(transaction)
    
    def get_category_keywords(self, category: str) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        """
        return self.category_keywords.get(category, [])

    def get_categories_by_text(self, text: str) -> List[Tuple[str, float]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º KeywordDictionary
        """
        return self.keyword_dict.get_categories_by_text(text)

    def get_category_by_keyword(self, keyword: str) -> Optional[Tuple[str, float]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º KeywordDictionary
        """
        return self.keyword_dict.get_category_by_keyword(keyword)

    def add_keyword(self, keyword: str, category: str, confidence: float = 0.5, save_to_sheet: bool = True):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤ KeywordDictionary
        """
        self.keyword_dict.add_keyword(keyword, category, confidence, save_to_sheet=save_to_sheet)

    def learn_keyword(self, text: str, category: str):
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–æ–≤–æ–º—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é —Ç–µ–∫—Å—Ç–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        –î–æ–±–∞–≤–ª—è–µ—Ç –ø–∞—Ä—É —Ç–µ–∫—Å—Ç-–∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Google Sheets.
        """
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞–µ–º —á–∏—Å–ª–∞
        cleaned_text = re.sub(r'\d+', '', text.lower().strip())
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–º –∏–ª–∏ –ø—É—Å—Ç—ã–º –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        if not cleaned_text or len(cleaned_text) < 2:
            return  # –ù–µ —É—á–∏–º—Å—è –Ω–∞ –ø—É—Å—Ç—ã—Ö –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ–º –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞
        if not re.search(r'[–∞-—è—ëa-z]', text.lower()):
            return # –ù–µ —É—á–∏–º—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –±–µ–∑ –±—É–∫–≤
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –∏–∑ KeywordDictionary
        normalized_text = self.keyword_dict.normalize_text(cleaned_text)
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω morph_analyzer
        lemmatized_text = self.lemmatize_text(cleaned_text) if self.morph_analyzer else normalized_text
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ KeywordDictionary
        self.add_keyword(normalized_text, category)
        
        # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        if lemmatized_text != normalized_text:
            self.add_keyword(lemmatized_text, category, save_to_sheet=False)  # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–µ–º–º—É –æ—Ç–¥–µ–ª—å–Ω–æ –≤ Google Sheets
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.categories.add(category)
        if hasattr(self, 'category_keywords'):
            if category not in self.category_keywords:
                self.category_keywords[category] = []
            if normalized_text not in self.category_keywords[category]:
                self.category_keywords[category].append(normalized_text)
            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –ª–µ–º–º—É, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
        if lemmatized_text != normalized_text and lemmatized_text not in self.category_keywords[category]:
                self.category_keywords[category].append(lemmatized_text)
        
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{normalized_text}' -> '{category}' (—Å –ª–µ–º–º–æ–π: '{lemmatized_text}')")
        self.save_model()

    def predict(self, text: str) -> str:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏–ª–∏ "–î—Ä—É–≥–æ–µ" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_result = self.get_category_by_keyword(text)
        if keyword_result:
            category, confidence = keyword_result
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤–∞–ª–∏–¥–Ω–∞ (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Å–∏—Å—Ç–µ–º–µ)
            if category in self.categories or self._is_valid_category(category):
                return category
        
        # –ï—Å–ª–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º ML-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        # –ù–æ –ø–µ—Ä–µ–¥ —ç—Ç–∏–º —Å–æ–∑–¥–∞–µ–º —Ñ–µ–π–∫–æ–≤—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ª–æ–≥–∏–∫–∏
        from models.transaction import TransactionData
        fake_transaction = TransactionData(
            type="–†–∞—Å—Ö–æ–¥",
            category="",  # –ü–æ–∫–∞ –Ω–µ –∑–Ω–∞–µ–º
            amount=0.0,
            comment=text,
            username="",
            retailer_name="",
            items_list="",
            payment_info="",
            transaction_dt=datetime.now()
        )
        
        predicted_category, confidence = self.predict_category(fake_transaction)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤–∞–ª–∏–¥–Ω–∞
        if predicted_category in self.categories or self._is_valid_category(predicted_category):
            return predicted_category
        else:
            # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –≤–∞–ª–∏–¥–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º "–î—Ä—É–≥–æ–µ" –∏–ª–∏ "–ù–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ"
            return "–î—Ä—É–≥–æ–µ"
    
    def _is_valid_category(self, category: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤–∞–ª–∏–¥–Ω–æ–π (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Å–∏—Å—Ç–µ–º–µ).
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –∏–ª–∏ –≤ KeywordDictionary
        return (category in self.categories or
                category in self.keyword_dict.category_keywords if hasattr(self.keyword_dict, 'category_keywords') else True)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
classifier = TransactionCategoryClassifier()